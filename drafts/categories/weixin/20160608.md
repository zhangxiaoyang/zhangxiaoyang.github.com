![](spider-wget.png)

wget，简单粗暴的下载器。

<!--more-->

## 1 组装爬虫

没有谁不想有一个属于自己的轮子，看到GitHub上飙升的Star和Fork，心里美滋滋的。那些用程序员鼓励师来博人眼球的文案，真是呵呵了。你们真不懂程序员，真正能给予我们力量的不是性感与裸露，而是来自用户、同行和社会的认可。

- 如果用户点赞了我们的产品；
- 如果同行提交了Pull Request；
- 如果社会上多几声工程师的称谓。

但是现实却有些骨感，制造一个满意的轮子往往是在梦醒时分。工作中，我们不得不压榨时间，留更多的时间给抓取的过程，让处理和分析走得更加顺畅。我们放弃了炫技的机会，争分夺秒，以最快的速度写出bug free的代码。然后运行代码，等待，等待，再等待，等待数据抓取完成。如果一不小心进坑，还要重新修复程序，再运行，再等待。

所以，我们希望尽快实现爬虫，不要和我提C++了，没时间了。什么，Python？还能不能更快？最好和堆积木一样，三下五除二组装起来，搞定。如果你还没有写过爬虫，那太好了，我们一起来用现成的工具堆一个吧！

![](duijimu.jpg)

## 2 下载器wget  

### 2.1 wget上手

wget，来源于	World Wide Web和get，是一个命令行工具。通过wget可以快速组装出如我心意的下载器，入门只需以下八步。

1) 抓取“zhangxiaoyang.me”可以这样做：

```
$ wget zhangxiaoyang.me
```

然后会将“zhangxiaoyang.me”对应的页面抓取并储存到本地当前路径，即`index.html`。

2) 如果我们想修改储存的文件名或路径，可以这样：

```
$ wget -O homepage.txt zhangxiaoyang.me # 修改文件名为homepage.txt
$ wget -P website zhangxiaoyang.me # 修改下载路径为website，没有该文件夹则自动创建
```

3) 作为一只有礼貌的爬虫，有必要控制一下下载速度，我们都不希望目标网站因承受不住而挂掉：

```
$ wget --limit-rate=200k zhangxiaoyang.me
```

4) 如果不幸网络断开或出现了异常情况导致中途下载失败了，不要怕，这在下载大文件的时候尤为重要：

```
$ wget -c zhangxiaoyang.me
```

5) 如果之前已经成功下载了，`wget -c`将不会重复下载，并提示我们：

```
The file is already fully retrieved; nothing to do.
```

6) 把下载任务丢到后台吧，我们可以搞其他的事情，wget还会在当前目录生成一个`wget-log`文件，里面记录了下载任务的详细情况：

```
$ wget -b zhangxiaoyang.me
Continuing in background, pid 2714.
Output will be written to ‘wget-log’.
```

7) 把自己伪装成浏览器吧，直接声称自己是爬虫是不是太实诚了：

````
$ wget -U "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.63 Safari/537.36" zhangxiaoyang.me
````

8) 以上参数可以随意组合，比如这样：

```
$ wget -U "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.63 Safari/537.36" -c -b --limit-rate=200k -O homepage.txt zhangxiaoyang.me
```

### 2.2 wget知新

1) 由于种种原因，有些URL不能一次抓取成功，可能需要多试几次（最多尝试3次）：

```
$ wget --tries=3 zhangxiaoyang.me
```

2) 设置连接超时（3秒）：

```
$ wget --connect-timeout=3 google.com
```

3) 我们有一批URL需要抓取，所有的URL储存在`urllist.txt`中，一行一个URL：

```
$ wget -i urllist.txt
```

4) 抓取一个FTP协议的URL，但是需要验证账号和密码：

```
$ wget --ftp-user=USERNAME --ftp-password=PASSWORD FTP_URL
```

5) 抓取一个HTTP协议的URL，但是需要验证账号和密码：

```
$ wget --http-user=USERNAME --http-password=PASSWORD HTTP_URL
```

6) 我只想抓取并输出，不要储存到文件：

```
$ wget -O- zhangxiaoyang.me
```

7) 输出的结果里带有下载进度等冗余信息，我不希望看到他们：

```
$ wget -q -O- zhangxiaoyang.me
```

8) 我似乎get了静默下载的新技能：

```
$ wget -q zhangxiaoyang.me
```

9) 把下载详情写入日志吧：

```
$ wget -o mylog zhangxiaoyang.me
```

### 2.3 wget抓站

1) 我要把“zhangxiaoyang.me”的所有内容都下载下来，我要收藏！

```
$ wget -r zhangxiaoyang.me
```

2) 如果之前已经抓过该站点，只想抓取有更新的内容：

```
$ wget -r -N zhangxiaoyang.me
```

3) 我只想下载“zhangxiaoyang.me”中的图片：

```
$ wget -r -A .jpg,.jpeg,.gif,.png zhangxiaoyang.me
```

4) 我不想下载图片：

```
$ wget -r -R .jpg,.jpeg,.gif,.png zhangxiaoyang.me
```

5) 设置域名来跟踪指定的域名：

```
$ wget -r -D zhangxiaoyang.me,vince67.github.io zhangxiaoyang.me
```

6) 限制批量下载时的时间间隔（3秒）：

```
$ wget -r -w 3 zhangxiaoyang.me
```

7) 设置抓取的深度，如果想无限抓取：

```
$ wget -r -l inf zhangxiaoyang.me
```

8) 镜像一个网站：

```
$ wget -r -N -l inf zhangxiaoyang.me
```

9) 如果想简化以上写法：

```
$ wget -m zhangxiaoyang.me
```

10) 把下载的HTML中的链接改为本地链接（指向下载的本地文件）：

```
$ wget -r -k zhangxiaoyang.me
```

当网站下载完成后，会执行转换，可以看到类似下面的输出：

```
FINISHED --2016-06-08 03:56:00--
Total wall clock time: 41s
Downloaded: 176 files, 7.3M in 2.6s (2.87 MB/s)
Converting links in zhangxiaoyang.me/categories/data-mining-tutorial/data-mining-tutorial-5-2.html... 23-2
Converting links in zhangxiaoyang.me/categories/weixin/20160606.html... 22-2
...
```

11) 下载所有和网站渲染相关的内容且他们必须隶属当前域名，比如图片、声音、样式，这样比用`-A`和`-R`方便很多：

```
$ wget -r -p zhangxiaoyang.me
```

12) 下载所有内容，哪怕这些内容在其他的域名下，这样比用`-D`方便很多：

```
$ wget -r -H zhangxiaoyang.me
```

13) 给下载的文件加上后缀（如果原来并没有文件后缀的话），比如一个HTML文件下载下来是无后缀的，此时给他加上`.html`：

```
$ wget -r -E zhangxiaoyang.me
```

14) 网站镜像终极大法：

```
$ wget -mpkEH zhangxiaoyang.me
```

15) 了解以上缩写参数的全称或者get更多技能：

```
$ wget --help
$ man wget
```

## 参考

- <http://unix.stackexchange.com/questions/47434/what-is-the-difference-between-curl-and-wget>
- <https://eyelublog.files.wordpress.com/2012/09/jimu.jpeg?w=640>
- <https://en.wikipedia.org/wiki/Wget>
- <http://www.thegeekstuff.com/2009/09/the-ultimate-wget-download-guide-with-15-awesome-examples/>
- <http://www.makeuseof.com/tag/mastering-wget-learning-neat-downloading-tricks/>
- <https://github.com/mcandre/cheatsheets/blob/master/wget.md>
- <https://gist.github.com/dannguyen/03a10e850656577cfb57>
